
@book{rfg,
	address = {New York},
	series = {Springer Monographs in Mathematics},
	title = {Random fields and geometry},
	isbn = {978-0-387-48112-8},
	url = {http://www.springer.com/978-0-387-48112-8},
	publisher = {Springer},
	author = {Adler, Robert J. and Taylor, Jonathan E.},
	year = {2007}
},



@article{lars,
	title = {Least angle regression},
	volume = {32},
	number = {2},
	journal = {Annals of Statistics},
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	year = {2004},
	pages = {407-–499}
},



@article{javanmard:montanari,
        author = {{Javanmard}, A. and {Montanari}, A.},
        title = {Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model: Asymptotic Theory},
        journal = {ArXiv e-prints},
        archivePrefix = "arXiv",
        eprint = {1301.4240},
        primaryClass = "stat.ME",
        keywords = {Statistics - Methodology, Computer Science - Information Theory, Mathematics - Statistics Theory, Statistics - Machine Learning},
        year = 2013,
        month = jan,
        adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1301.4240J},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
},



@article{significance:lasso,
	title = {A significance test for the lasso},
	url = {http://arxiv.org/abs/1301.7161},
	abstract = {In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the \{{\textbackslash}it covariance test statistic\}, and show that when the true model is linear, this statistic has an \${\textbackslash}Exp(1)\$ asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result assumes some (reasonable) regularity conditions on the predictor matrix {\$X\$}, and covers the important high-dimensional case \$p{\textgreater}n\$. Of course, for testing the significance of an additional variable between two nested linear models, one may use the usual chi-squared test, comparing the drop in residual sum of squares ({RSS)} to a \${\textbackslash}chi{\textasciicircum}2\_1\$ distribution. But when this additional variable is not fixed, but has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in {RSS} stochastically much larger than \${\textbackslash}chi{\textasciicircum}2\_1\$ under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter \${\textbackslash}lambda\$ decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the \${\textbackslash}ell\_1\$ penalty. Therefore the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties---adaptivity and shrinkage---and its null distribution is tractable and asymptotically \${\textbackslash}Exp(1)\$.},
	urldate = {2013-01-31},
	journal = {{arXiv:1301.7161}},
	author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan and Tibshirani, Robert},
	month = jan,
	year = {2013},
	keywords = {{62J07}, {62F03}, Mathematics - Statistics Theory, Statistics - Methodology},
        note = {Submitted to Annals of Statistics}
},


@article{tests:adaptive,
        author = {{Taylor}, J. and {Loftus}, J. and {Tibshirani}, R.},
        title = {Tests in adaptive regression via the Kac-Rice formula},
        journal = {ArXiv e-prints},
        archivePrefix = "arXiv",
        eprint = {1308.3020},
        primaryClass = "stat.ME",
        keywords = {Statistics - Methodology},
        year = 2013,
        month = aug,
        adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.3020T},
        adsnote = {Provided by the SAO/NASA Astrophysics Data System}
},



@article{taylor:validity,
	title = {Validity of the expected Euler characteristic heuristic},
	volume = {33},
	issn = {0091-1798},
	number = {4},
	journal = {The Annals of Probability},
	author = {Taylor, {J.E.} and Takemura, A. and Adler, {R.J.}},
	year = {2005},
	pages = {1362-–1396}
},



@article{tibshirani:lasso,
	title = {Regression shrinkage and selection via the lasso},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267-–288}
},



@article{grouplasso,
	title = {Model selection and estimation in regression with grouped variables},
	volume = {68},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B},
	author = {Ming, Yuan and Lin, Yi},
	year = {2005},
	pages = {49-–67}
},


@ARTICLE{sequential:fdr,
   author = {{Grazier G'Sell}, M. and {Wager}, S. and {Chouldechova}, A. and {Tibshirani}, R.},
   title = "{False Discovery Rate Control for Sequential Selection Procedures, with Application to the Lasso}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1309.5352},
 primaryClass = "math.ST",
 keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
     year = 2013,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1309.5352G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@article{RIC,
     jstor_articletype = {research-article},
     title = {The Risk Inflation Criterion for Multiple Regression},
     author = {Foster, Dean P. and George, Edward I.},
     journal = {The Annals of Statistics},
     jstor_issuetitle = {},
     volume = {22},
     number = {4},
     jstor_formatteddate = {Dec., 1994},
     pages = {pp. 1947-1975},
     url = {http://www.jstor.org/stable/2242493},
     ISSN = {00905364},
     abstract = {A new criterion is proposed for the evaluation of variable selection procedures in multiple regression. This criterion, which we call the risk inflation, is based on an adjustment to the risk. Essentially, the risk inflation is the maximum increase in risk due to selecting rather than knowing the "correct" predictors. A new variable selection procedure is obtained which, in the case of orthogonal predictors, substantially improves on AIC, Cp and BIC and is close to optimal. In contrast to AIC, Cp and BIC which use dimensionality penalties of 2, 2 and log n, respectively, this new procedure uses a penalty 2 log p, where p is the number of available predictors. For the case of nonorthogonal predictors, bounds for the optimal penalty are obtained.},
     language = {English},
     year = {1994},
     publisher = {Institute of Mathematical Statistics},
     copyright = {Copyright © 1994 Institute of Mathematical Statistics},
     annote = {Risk inflation improves on Cp, AIC, BIC (if ortho X), using 2log(p)}
}

@article{BIC,
     jstor_articletype = {research-article},
     title = {Estimating the Dimension of a Model},
     author = {Gideon Schwarz},
     journal = {The Annals of Statistics},
     jstor_issuetitle = {},
     volume = {6},
     number = {2},
     jstor_formatteddate = {Mar., 1978},
     pages = {pp. 461-464},
     url = {http://www.jstor.org/stable/2958889},
     ISSN = {00905364},
     abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
     language = {English},
     year = {1978},
     publisher = {Institute of Mathematical Statistics},
     copyright = {Copyright © 1978 Institute of Mathematical Statistics},
     annote = {First BIC paper},
}

@article{CP,
     jstor_articletype = {research-article},
     title = {Some Comments on CP},
     author = {Mallows, C. L.},
     journal = {Technometrics},
     jstor_issuetitle = {},
     volume = {15},
     number = {4},
     jstor_formatteddate = {Nov., 1973},
     pages = {pp. 661-675},
     url = {http://www.jstor.org/stable/1267380},
     ISSN = {00401706},
     abstract = {We discuss the interpretation of CP-plots and show how they can be calibrated in several ways. We comment on the practice of using the display as a basis for formal selection of a subset-regression model, and extend the range of application of the device to encompass arbitrary linear estimates of the regression coefficients, for example Ridge estimates.},
     language = {English},
     year = {1973},
     publisher = {American Statistical Association and American Society for Quality},
     copyright = {Copyright © 1973 American Statistical Association and American Society for Quality},
}

@ARTICLE{AIC, 
  author={Akaike, H.}, 
  journal={Automatic Control, IEEE Transactions on}, 
  title={A new look at the statistical model identification}, 
  year={1974}, 
  volume={19}, 
  number={6}, 
  pages={716-723}, 
  abstract={The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.}, 
  keywords={Parameter identification;Time series;maximum-likelihood (ML) estimation;Art;Estimation theory;History;Linear systems;Maximum likelihood estimation;Roundoff errors;Sampling methods;Stochastic processes;Testing;Time series analysis}, 
  doi={10.1109/TAC.1974.1100705}, 
  ISSN={0018-9286},
},


@ARTICLE{donoho:pursuit, 
author={Donoho, D.L. and Elad, M. and Temlyakov, V.N.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Stable recovery of sparse overcomplete representations in the presence of noise}, 
year={2006}, 
volume={52}, 
number={1}, 
pages={6-18}, 
keywords={approximation theory;iterative methods;signal denoising;signal representation;time-frequency analysis;Kruskal rank;greedy approximation algorithm;incoherent dictionary;matching pursuit;noisy data;optimal sparse decomposition;signal processing theory;sparse overcomplete representation;stable recovery;stepwise regression;superresolution signal;Dictionaries;Linear algebra;Matching pursuit algorithms;Noise generators;Noise level;Signal processing;Signal processing algorithms;Signal representations;Stability;Vectors;Basis pursuit;Kruskal rank;greedy approximation;incoherent dictionary;matching pursuit;overcomplete representation;sparse representation;stability;stepwise regression;superresolution}, 
doi={10.1109/TIT.2005.860430}, 
ISSN={0018-9448},
annote={Donoho paper, matching pursuit can give perfect recovery under an incoherence assumption, highly cited},
}

@ARTICLE{matching:pursuit, 
author={Mallat, S.G. and Zhang, Z.}, 
journal={Signal Processing, IEEE Transactions on}, 
title={Matching pursuits with time-frequency dictionaries}, 
year={1993}, 
volume={41}, 
number={12}, 
pages={3397-3415}, 
keywords={signal processing;time-frequency analysis;wavelet transforms;Gabor functions;adaptive signal representations;adaptive time-frequency transform;linear waveform expansion;matching pursuit algorithm;matching pursuit decomposition;noisy signals;optimized wavepacket orthonormal basis;pattern extraction;signal energy distribution;signal expansion;signal structures;time-frequency dictionaries;time-frequency plane;Dictionaries;Fourier transforms;Interference;Matching pursuit algorithms;Natural languages;Pursuit algorithms;Signal processing algorithms;Signal representations;Time frequency analysis;Vocabulary}, 
doi={10.1109/78.258082}, 
ISSN={1053-587X},
annote={Signal processing community, origin of name matching pursuit, highly cited, overcomplete dictionaries of wave functions},
}

@article{fdr,
  title={Controlling the false discovery rate: a practical and powerful approach to multiple testing},
  author={Benjamini, Yoav and Hochberg, Yosef},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={289--300},
  year={1995},
  publisher={JSTOR}
}

@ARTICLE{glint,
   author = {{Lim}, M. and {Hastie}, T.},
    title = "{Learning interactions through hierarchical group-lasso regularization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1308.2719},
 primaryClass = "stat.ME",
 keywords = {Statistics - Methodology},
     year = 2013,
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.2719L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{mc:ftoenter,
     jstor_articletype = {research-article},
     title = {Tests of Significance in Forward Selection Regression with an F-to-Enter Stopping Rule},
     author = {Leland Wilkinson and Dallal, Gerard E.},
     journal = {Technometrics},
     jstor_issuetitle = {},
     volume = {23},
     number = {4},
     jstor_formatteddate = {Nov., 1981},
     pages = {pp. 377-380},
     url = {http://www.jstor.org/stable/1268227},
     ISSN = {00401706},
     abstract = {A Monte Carlo simulation is used to estimate the upper percentage points of the null distribution of the sample squared multiple correlation coefficient (R<sup>2</sup>) when the number of predictors selected is determined by a stopping rule. In the study, the sample size n and the number of candidate predictors m satisfy 2 ≤ m ≤ 20 and 10 ≤ n - m - 1 ≤ 200, while the F threshold ranges from two to four. Tables of the upper five percent and upper one percent sample R<sup>2</sup> values are presented and an example is given to illustrate the use of the tables.},
     language = {English},
     year = {1981},
     publisher = {American Statistical Association and American Society for Quality},
     copyright = {Copyright © 1981 American Statistical Association and American Society for Quality},
     annote = {Computed (by MC) tables of R<sup>2</sup> values from models selected with FS with F-to-enter stopping rule},
}

@article{classical:selection,
     jstor_articletype = {research-article},
     title = {A Biometrics Invited Paper. The Analysis and Selection of Variables in Linear Regression},
     author = {Hocking, R. R.},
     journal = {Biometrics},
     jstor_issuetitle = {},
     volume = {32},
     number = {1},
     jstor_formatteddate = {Mar., 1976},
     pages = {pp. 1-49},
     url = {http://www.jstor.org/stable/2529336},
     ISSN = {0006341X},
     abstract = {},
     language = {English},
     year = {1976},
     publisher = {International Biometric Society},
     copyright = {Copyright © 1976 International Biometric Society},
     annote = {An old reference on FS and other selection methods},
}

@article{permutation:stop,
author = {Forsythe, Alan B. and Engelman, Laszlo and Jennrich, Robert and May, Philip R. A.},
title = {A Stopping Rule for Variable Selection in Multiple Regression},
journal = {Journal of the American Statistical Association},
volume = {68},
number = {341},
pages = {75-77},
year = {1973},
doi = {10.1080/01621459.1973.10481336},

URL = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1973.10481336},
abstract = { Abstract Stepwise regression is a member of a class of statistical methods which may be called opportunistic. The statistician is naturally concerned that an artifact should not be reported so he needs a stopping rule that is geared to the way in which variables are selected. A sampled permutation test is outlined which offers such a stopping rule for forward stepping. },
annote = {Early permutation based stopping rule to control greediness}
}

@article{wasserman:roeder,
	title = {High-dimensional variable selection},
	volume = {37},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1247663752},
	doi = {10.1214/08-AOS646},
	abstract = {This paper explores the following question: what kind of statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, we look at the error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as “screening” and the last stage as “cleaning.” We consider three screening methods: the lasso, marginal regression, and forward stepwise regression. Our method gives consistent variable selection under certain conditions.},
	language = {{EN}},
	number = {5},
	urldate = {2013-01-31},
	journal = {The Annals of Statistics},
	author = {Wasserman, Larry and Roeder, Kathryn},
	month = oct,
	year = {2009},
	note = {Zentralblatt {MATH} identifier: 05596898; Mathematical Reviews number ({MathSciNet):} {MR2543689}},
	pages = {2178--2201}
}
                  
@article{overlap:group:lasso,
	title = {Group Lasso with Overlaps: the Latent Group Lasso approach},
	shorttitle = {Group Lasso with Overlaps},
	url = {http://arxiv.org/abs/1110.0413},
	abstract = {We study a norm for structured sparsity which leads to sparse linear predictors whose supports are unions of prede ned overlapping groups of variables. We call the obtained formulation latent group Lasso, since it is based on applying the usual group Lasso penalty on a set of latent variables. A detailed analysis of the norm and its properties is presented and we characterize conditions under which the set of groups associated with latent variables are correctly identi ed. We motivate and discuss the delicate choice of weights associated to each group, and illustrate this approach on simulated data and on the problem of breast cancer prognosis from gene expression data.},
	urldate = {2013-01-31},
	journal = {{arXiv:1110.0413}},
	author = {Obozinski, Guillaume and Jacob, Laurent and Vert, Jean-Philippe},
	month = oct,
	year = {2011},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{buhlmann,
	title = {Statistical significance in high-dimensional linear models},
	url = {http://arxiv.org/abs/1202.1377},
	abstract = {We propose a method for constructing p-values for general hypotheses in a high-dimensional linear model. The hypotheses can be local for testing a single regression parameter or they may be more global involving several up to all parameters. Furthermore, when considering many hypotheses, we show how to adjust for multiple testing taking dependence among the p-values into account. Our technique is based on Ridge estimation with an additional correction term due to a substantial projection bias in high dimensions. We prove strong error control for our p-values and provide sufficient conditions for detection: for the former, we do not make any assumption on the size of the true underlying regression coefficients while regarding the latter, our procedure might not be optimal in terms of power. We demonstrate the method in simulated examples and a real data application.},
	urldate = {2013-01-31},
	journal = {{arXiv:1202.1377}},
	author = {B\"uhlmann, Peter},
	month = feb,
	year = {2012},
	keywords = {{62J07} (Primary), {62F03} (Secondary), Mathematics - Statistics Theory, Statistics - Methodology},
}

@article{meinshausen:buhlmann,
	title = {Stability selection},
	volume = {72},
	copyright = {© 2010 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/j.1467-9868.2010.00740.x},
	abstract = {Summary. Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
	language = {en},
	number = {4},
	urldate = {2013-01-31},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meinshausen, Nicolai and B\"uhlmann, Peter},
	year = {2010},
	keywords = {High dimensional data, Resampling, Stability selection, Structure estimation},
	pages = {417-–473},
}

@article{bien:hierarchical,
	title = {A Lasso for Hierarchical Interactions},
	url = {http://arxiv.org/abs/1205.5050},
	abstract = {We add a set of convex constraints to the Lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one, and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting "saved" by the hierarchy constraint. We distinguish between parameter sparsity-the number of nonzero coefficients-and practical sparsity-the number of raw variables one must measure to make a new prediction. Hierarchy gets at the latter, which is more closely tied to important data collection concerns such as cost, time, and effort. We develop an algorithm, available in the R package {hierNet}, and perform an empirical study of our method.},
	urldate = {2012-10-23},
	journal = {{arXiv:1205.5050}},
	author = {Bien, Jacob and Taylor, Jonathan and Tibshirani, Robert},
	month = may,
	year = {2012},
	note = {Submitted to Annals of Statistics.},
	keywords = {{JT2013}, Statistics - Machine Learning, Statistics - Methodology},
}

@ARTICLE{cai:wang:omp, 
  author={Cai, T.T. and Lie Wang}, 
  journal={Information Theory, IEEE Transactions on}, 
  title={Orthogonal Matching Pursuit for Sparse Signal Recovery With Noise}, 
  year={2011}, 
  month={July}, 
  volume={57}, 
  number={7}, 
  pages={4680-4688}, 
  abstract={We consider the orthogonal matching pursuit (OMP) algorithm for the recovery of a high-dimensional sparse signal based on a small number of noisy linear measurements. OMP is an iterative greedy algorithm that selects at each step the column, which is most correlated with the current residuals. In this paper, we present a fully data driven OMP algorithm with explicit stopping rules. It is shown that under conditions on the mutual incoherence and the minimum magnitude of the nonzero components of the signal, the support of the signal can be recovered exactly by the OMP algorithm with high probability. In addition, we also consider the problem of identifying significant components in the case where some of the nonzero components are possibly small. It is shown that in this case the OMP algorithm will still select all the significant components before possibly selecting incorrect ones. Moreover, with modified stopping rules, the OMP algorithm can ensure that no zero components are selected.}, 
  doi={10.1109/TIT.2011.2146090}, 
  ISSN={0018-9448},
}


@article{cai:wang:xu:sharp,
  author = {Cai, Tony Tony and Wang, Lie and Xu, Guangwu},
  title = {Stable Recovery of Sparse Signals and an Oracle Inequality},
  journal = {IEEE Trans. Inf. Theor.},
  issue_date = {July 2010},
  volume = {56},
  number = {7},
  month = jul,
  year = {2010},
  issn = {0018-9448},
  pages = {3516--3522},
  numpages = {7},
  url = {http://dx.doi.org/10.1109/TIT.2010.2048506},
  doi = {10.1109/TIT.2010.2048506},
  acmid = {1840531},
  publisher = {IEEE Press},
  address = {Piscataway, NJ, USA},
} 