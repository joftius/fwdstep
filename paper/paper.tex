% Template for the submittion to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos
%\documentclass[aos]{imsart}
\documentclass{imsart}

\usepackage{amsthm,amsmath,natbib}
%\RequirePackage[dvips]{hyperref}

% use this package if hyperref and natbib is used:
%\RequirePackage{hypernat}

% provide arXiv number if available:
%\arxiv{math.PR/0000000}

% put your definitions there:
\startlocaldefs
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphics,amssymb,color}
\usepackage{graphicx,subfigure}
%\usepackage[boxed]{algorithm2e}


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}}}

\newcommand{\todo}{\textcolor{red}{\textbf{To do: }}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\Pp}{{\mathbb P}}
\newcommand{\Ee}{{\mathbb E}}
\newcommand{\lips}{{\cal L}}
\newcommand{\tube}{\mathrm{Tube}}
\newcommand{\sphere}[1]{O^{#1-1}}
\newcommand{\dimens}{\text{dim}}
\newcommand{\cone}{\text{Cone}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newcommand{\XK}{K_X}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\innerp}[2]{\langle #1 , #2 \rangle}
%\newcommand{\qed}{\hfill $\Box$\newline}

\newcommand{\grad}{\nabla}
\newcommand{\V}{\mathcal{V}}\endlocaldefs
\newcommand{\K}{\mathcal{K}}
\newcommand\tf{\widetilde{f}}
\newcommand{\pen}{\mathcal{P}}
\newcommand{\vecsp}{\mathcal{L}}
\newcommand{\gstar}{g^*}

\begin{document}

\begin{frontmatter}

% "Title of the paper"
\title{A significance test in forward stepwise regression}

%%%
%%% \runtitle{} - I don't know what this is
%%%

% indicate corresponding author with \corref{}
%\author{\fnms{Jonathan} \snm{Taylor}\corref{Jonathan E. Taylor}\ead[label=e1]{jonathan.taylor@stanford.edu}\thanksref{t1}
%\and \fnms{Joshua} \snm{Loftus}}
%\thankstext{t1}{Supported in part by NSF grant DMS 1208857 and AFOSR grant 113039.}

\affiliation{Stanford University}

\address{Department of Statistics\\  Stanford University\\ Sequoia
Hall \\390 Serra Mall\\ Stanford, CA 94305, U.S.A.\\ }
%\printead{e1}}

%%%
%%% \runauthor{Taylor} - I don't know what this is
%%%

\begin{abstract}
% perhaps de-emphasize group lasso and emphasize forward stepwise?
  We extend the methods developed by \cite{significance:lasso} and
  \cite{tests:adaptive} on significance tests for penalized
  regression to stepwise model selection by iteratively applying
  the global null hypothesis test on the residual in a forward stepwise
  procedure. While not fully theoretically justified, the resulting method
  has the computational strengths of stepwise selection and partially
  solves the problem of inflated test statistics due to model selection.
  We illustrate the flexibility of this method by applying it to
  novel specialized applications of forward stepwise to a hierarchically
  constrained interactions model and generalized additive models.
\end{abstract}

%%%
%%% - Gotta change these??????????
%%%
\begin{keyword}[class=AMS]
\kwd[Primary ]{62M40}
%\kwd{}
\kwd[; secondary ]{62H35}
\end{keyword}

\begin{keyword}
\kwd{forward stepwise}
\kwd{model selection}
\kwd{significance test}
\end{keyword}

\end{frontmatter}


\section{Introduction}
\label{sec:intro}

Forward stepwise regression is a classical model selection procedure
that begins with an empty model and sequentially adds the best predictor
variable in each step. Classical significance tests fail
when a model
has been selected this way and tend to be anti-conservative.
In the lasso setting, \cite{significance:lasso} derived a novel test
statistic along with a correct asymptotic distribution, making possible
valid inferences after model selection.
\cite{tests:adaptive} modified and extended those results to the
group lasso \citep{grouplasso} and other penalized regression
problems, but only under the global null hypothesis.
One of the strengths of these test statistics is that they can be
used for valid significance testing when computed on the same
data used for model selection, eliminating the need for data splitting.
The present work iteratively applies the global null test
of \cite{tests:adaptive} for each step in forward stepwise selection,
and works out some of the details necessary for
models with grouped variables. The resulting method can
be more statistically efficient than validation on held-out data, and
more computationally efficient than penalized methods with
regularization parameters chosen by cross-validation.


In Section~\ref{sec:stepwise} we establish notation and describe the
forward stepwise algorithm used in most of the paper.
Section~\ref{sec:testing} briefly reviews
the parts of \cite{significance:lasso} and \cite{tests:adaptive}
relevant to our significance test, and describes the group lasso which
we require for applying the test with grouped variables.
Simulation results in Section~\ref{sec:simulations} show
empirically that our method performs well in settings where forward
stepwise itself performs well, and that
various stopping rules using our test statistic--including
some from \cite{sequential:fdr}--appear promising. In
Section~\ref{sec:applications} we apply
the method to several variants of forward
stepwise tailored to models with interactions and generalized additive
models, as well as to a real data example.
(\todo Change this paragraph if necessary as the sections are completed)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Forward stepwise model selection}
\label{sec:stepwise}

\subsection{Background and notation}

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{../figs/pubmed.pdf}
\caption{\small \it Forward stepwise enjoys widespread use among
practitioners}
\label{fig:pubmed}
\end{center}
\end{figure}


As a classical method dating back about half a century
(see \cite{classical:selection} for a review),
forward stepwise regression has not received much attention in
recent years in the theoretical statistics community. But it continues
to be widely used by practitioners.
For example, search results on PubMed for forward stepwise
(summarized in Figure~\ref{fig:pubmed}) show that many recent
papers mention the method and there is an increasing trend over time.
Its popularity among researchers continues despite the fact that it
invalidates inferences using the standard $\chi^2$ or $F$-tests.


Some classical attempts to address this issue include Monte Carlo
estimation of tables of adjusted $F$-statistic values \citep{mc:ftoenter},
and permutation statistics \citep{permutation:stop}. Aside from the works
this paper is based on, there have been other recent attempts to do
inference after model selection. Most of these make use of subsampling
\citep{meinshausen:buhlmann} or data splitting \citep{wasserman:roeder}.
Our approach allows use of the full data and does not require the
extra computation involved in subsampling.
Before describing the full approach we first introduce notation and
specify our implementation of forward stepwise, which is slightly
different from the most commonly used versions.


We allow forward stepwise selection to add groups of variables in each
step, not only in the case of binary encoding for categorical
variables but also for any grouping purpose. For example, groups of
variables may be pre-designated factors such as expression
measurements for all genes in a single functional pathway.
To emphasize this we will use $g, h$ as indices rather than the usual
$i, j$ throughout. Since single variables can be considered groups of
size 1, our general exposition includes non-grouped situations as a special
case. Our variant of forward stepwise uses a different method than
usual for choosing which group to add because we are using the
same objective in the forward stepwise procedure that was used to
derive the test statistics in penalized regression settings.

Denote $y \in \real^n$ for $n$ i.i.d. measurements of the outcome variable.
Let an integer $G \geq 2$ be the number of groups of explanatory variables.
For each $1 \leq g \leq G$ the design matrix encoding the
$g$th group is the $n \times p_g$ matrix denoted $X_g$, where $p_g$ is
the number of individual variables or columns in group $g$.
When a group encodes a categorical variable as binary indicators for the
levels of that variable, we use the full encoding with a column for every
level. Although this introduces collinearity, our method does not require
the design matrix to have full rank.


Define $p = \sum_{g=1}^Gp_g$,
the total number of individual variables, so $p = G$ in the
case where all groups have size 1. Let $X$ be the matrix constructed
by column-binding the $X_g$, that is  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation*}
X = \begin{pmatrix} X_1 & X_2 & \cdots & X_G  \end{pmatrix}
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We assume throughout that the individual columns of $X$ are scaled to
have unit 2-norm (we do not scale groups, this is accomplished later
by weighting).
With each group we associate the $p_g \times 1$ coefficient vector
$\beta_g$, and write $\beta$ for the $p \times 1$ vector constructed
by stacking all of the $\beta_g$ in order.  Finally, our model for the
response is
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
\label{eq:gmodel}
y & = X \beta + \sigma \epsilon \\
   & = \sum_{g=1}^G X_g \beta_g + \sigma \epsilon
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $\epsilon$ is noise. Unless otherwise specified we assume
i.i.d. Gaussian noise $\epsilon \sim N(0, I_{n \times n})$ and that
$\sigma$ is known.

We allow for the possibility $p > n$ in which case \eqref{eq:gmodel} is
generically underdetermined. In such cases it still may be possible to
estimate $\beta$ well if it is sparse--that is, if it has few nonzero
entries. In the rest of this paper we refer to variable groups $X_g$
as noise groups if $\beta_g$ is a zero vector and as true or signal
groups if $\beta_g$ has any nonzero entries. We refer to the number
of such nonzero groups as the \textit{sparsity} of the model, and
denote this $k := \# \{ g : \beta_g \neq 0 \}$.

Before we describe the forward stepwise procedure we require one last
ingredient. To each group of variables we assign a weight $w_g$. These
weights act like penalties or costs, so increasing $w_g$ makes it
more difficult for the group $X_g$ to enter the model. The
modeler can choose weights arbitrarily, but we will mostly use one
particular choice, based on $p_g$, that we discuss later. With this we
are ready to describe the forward stepwise procedure.


\subsection{Description of the algorithm}

First, the user must specify the maximum number of steps allowed,
which we denote \textit{steps} (consistent with the \textit{R} function
for stepwise). To enable our test statistic computations,
\textit{steps} should be at most $\min (n, G) - 1$, but it is
computationally desirable to set it as low as possible while still being
larger than the (unknown) sparsity of $\beta$.
This way forward stepwise has a chance of recovering all the nonzero
coefficients of $\beta$ and then terminating without performing much
additional computation.
In our implementation we treat the active set $A$ as an ordered list to easily track the order of groups entering the model.

% \begin{algorithm}
% \DontPrintSemicolon
% \KwData{An $n$ vector $y$ and $n \times p$ matrix $X$ of $G$ groups of variables}
% \KwResult{Active set $A$ of variable groups included in the model}
% \SetKwFunction{lsfitResidual}{lsfitResidual}
% \caption{\small \it Forward stepwise procedure with groups and weights}
% \BlankLine
% $A \gets \emptyset$\;
% $A^c \gets \{ 1, \ldots, G \}$\;
% $r_0 \gets y$\;
% \For{$s \gets 1$ \KwTo $steps$}{
%   $g^* \gets \argmax_{g \in A^c} \{ \norm{X_g^T r_{s-1}}_2 / w_g \}$\;
%   $P \gets I - X_{g^*}X_{g^*}^\dagger$\;
%   $A \gets A \cup \{ g^* \}$\;
%   $A^c \gets A^c \backslash \{ g^* \}$\;
%   \For{$g \in A^c$}{
%     $X_g \gets P X_g$\;
%   }
%   $r_s \gets P r_{s-1}$\;
% }
% \KwRet{$A$}
% \label{algo:fs}
% \end{algorithm}

\begin{algorithm}
  \caption{Forward stepwise variant with groups and weights}
  \label{algo:fs}
  \begin{algorithmic}[1]
    \REQUIRE An $n$ vector $y$ and $n \times p$ matrix $X$ of $G$ variable groups with weights $w_g$
    \ENSURE Ordered active set $A$ of variable groups included in the model at each step
    \STATE $A \gets \emptyset$, $A^c \gets \{ 1, \ldots, G\}$
    \FOR{$s=1$ to $steps$}
    \STATE $g^* \gets \argmax_{g \in A^c} \{ \norm{X_g^T r_{s-1}}_2 / w_g \}$
    \STATE $P \gets I - X_{g^*}X_{g^*}^\dagger$
    \STATE $A \gets A \cup \{ g^* \}$, $A^c \gets A^c \backslash \{ g^* \}$
    \FORALL{$g \in A^c$}
      \STATE $X_g \gets P X_g$
    \ENDFOR
    \STATE $r_s \gets P r_{s-1}$
    \ENDFOR
    \RETURN $A$
  \end{algorithmic}
\end{algorithm}

In the common case where all groups are single variables this version
coincides with the standard implementations of forward stepwise.
We will now use forward stepwise to refer specifically to
Algorithm~\ref{algo:fs} unless otherwise specified. Note that other
implementations of forward stepwise use different criteria for choosing
the next variable to add, such as the correlation with the residual.
Since we do not renormalize the columns after projecting the covariates
(lines 6 to 8 above), and since we have weights, we are not computing
correlations unless the design matrix is orthogonal and all weights are 1.
There are advantages and disadvantages to both criteria which we do not
discuss. Our choice was motivated by the group lasso result in
\cite{tests:adaptive}, but we believe other criteria can be handled
with appropriate modifications.

\subsection{Performance of forward stepwise}

Among model selection procedures, forward stepwise is one which performs
variable selection: from a potentially large set of variables it chooses
a subset to include in the model. The most general form of variable
selection is called subset selection, a procedure which searches among
all $2^G$ subsets of $G$ variable groups and picks the best model.
This exhaustive search is computationally infeasible when $G$ is
large, and when possible it still
runs the risk of over-fitting unless model complexity is
appropriately penalized (as in \eqref{eq:subsetregress} below).
Forward stepwise produces a much
smaller set of potential models, with cardinality at most
\textit{steps}. However
it is a greedy algorithm, so the set of models it produces may not
contain the best possible model. This is an inherent shortcoming of
forward stepwise procedures and should be kept in mind when choosing
between model selection methods.

So far we have left open the question of choosing among the models in
the forward stepwise sequence, i.e. when to stop stepping
forward. Some approaches for this problem can be posed as optimization
criteria which stop at the step minimizing
\begin{equation}
\begin{aligned}
\label{eq:subsetregress}
\frac{1}{2} \| y - X \beta_s \|_2^2 + \lambda \pen(\beta_s)
\end{aligned}
\end{equation}
(\todo describe construction of $\beta$ after finding $A$)
where we have written $\{ \beta_s : s = 1, \ldots, steps \}$ as
the sequence of models outputted by forward stepwise. The function
$\pen(\beta)$ is a penalty on model complexity usually taken to be the
number of nonzero entries of $\beta$. Proposals for $\lambda$ include
2 ($C_p$ of \cite{CP}, AIC of \cite{AIC}), $\log(n)$ (BIC of \cite{BIC}), and
$2\log(p)$ (RIC of \cite{RIC}). Stopping rules based on classical test
statistics have also been used, so it is natural to consider using
the new test statistics of \cite{significance:lasso} or
\cite{tests:adaptive}
to choose a model. \cite{sequential:fdr} examined some stopping rules
using the asymptotic p-values of \cite{significance:lasso} and showed
their stopping rules control false discovery rate--the expected
proportion of noise variables among variables declared significant
\citep{fdr}. We explore this further in Section~\ref{sec:simulations}.

Although forward stepwise is a greedy algorithm producing a
potentially sub-optimal sequence of models, under favorable conditions
it can still perform well. There is a small segment of the
compressed sensing literature \citep{donoho:pursuit, cai:wang:omp}
with results stating that forward stepwise (often referred to in that
literature as Orthogonal Matching Pursuit or OMP) can exactly select
the true model
under some stringent conditions involving quantities like the sparsity
of the true model and the coherence of the design matrix. The
coherence $\mu(X)$ of a matrix $X$ with columns scaled to have unit
2-norm is defined as

\begin{equation}
  \mu := \mu(X) = \max_{i \neq j} \{ | \innerp{ X_i }{ X_j } | \}
\end{equation}

Recalling that $k$ is the sparsity of $\beta$, typical results in this
literature say that if $k < (1/\mu + 1)/2$ and the nonzero
coefficients of $\beta$ are sufficiently large then forward stepwise
recovers $\beta$ with high probability. The coherence condition is
necessary to guarantee exact recovery \citep{cai:wang:xu:sharp} in the
sense that it is possible to construct counterexamples with
$k = (1/\mu + 1)/2$. We refer the reader to the literature for details.
For our purposes the
conditions required to guarantee exact recovery are usually too
stringent. Simulations show empirically that forward
stepwise can work well even when it is not working perfectly, and that
it does so under a wide range of conditions.


\begin{figure}
\begin{center}
\subfigure[Ternary design]{
\label{fig:fsternary}
\includegraphics[width=0.5\textwidth]{../figs/fwdstep/ternary_p2n_n100_lower1p4_upper1p4.pdf}}
\hspace{-15pt}
\subfigure[Gaussian design with correlation]{
\label{fig:fsgausscorr}
\includegraphics[width=0.5\textwidth]{../figs/fwdstep/gaussian_p2n_n100_lower1p4_upper1p6_corr0p2.pdf}}
\caption{\small \it The left panel shows the results of the simulation
  using design matrices with i.i.d. ternary entries taking values $0,
  \pm 1$ with equal probability. The left panel shows the results
  with design matrices of standard Gaussian entries with
  equi-correlation of 0.2. }
\label{fig:fwdstepsim}
\end{center}
\end{figure}

For various sparsity levels $k$, Algorithm~\ref{algo:fs} was run on
various data sets with \textit{steps} equal to $k$ and the ``average
power'' was calculated as the number of true variables in the active
set after $k$ steps divided by $k$. Nonzero coefficients had magnitude
in a range of multiples of $\gamma := \sqrt{2\log(G)}$, e.g. in $[1.4
\gamma, 1.6 \gamma]$. Results are shown in
Figure~\ref{fig:fwdstepsim}. After computing the
coherence of these design matrices, some calculations show the required
sparsity level to guarantee exact recovery in these situations is
about 2 or smaller, and the required nonzero coefficient magnitude is
likely in the range of 10 to 100 times $\gamma$. These simulations are far
from the stringent conditions required by theory to guarantee perfect
recovery, but the performance, while not perfect, may still be good
enough for some purposes.



%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Significance testing: from lasso to group lasso}
\label{sec:testing}

\todo Reorganize and update this section \\

In the ordinary least squares setting, a significance test for a
single variable can be conducted by comparing the drop in residual
sums of squares (RSS) to a $\chi^2_1$ distribution. Similarly, when
adding a group of $k$ variables we can compare the drop in RSS to a
$\chi^2_k$ random variable. This generally does not work when the
group to be added has been chosen by a method that uses the data,
and in particular it fails for forward stepwise procedures adding
the ``best'' (e.g. most highly correlated) predictor in each step. In
that case, the test statistic (drop in RSS) does not match the
theoretical null distribution even when the null hypothesis is
true. \cite{significance:lasso} introduced a new test statistic based
on the knots in the lasso solution path. They derived a simple
asymptotic null distribution, proved a type of convergence under broad
``minimum growth'' conditions, and demonstrated in simulations that
the test statistic closely matches its asymptotic distribution even in
finite samples.  That work marked an important advance in the problem
of combining inference with model selection.  \cite{tests:adaptive}
extended that work to the group lasso \citep{grouplasso} and other
problems, and modified the test statistic to one with an exact finite
sample distribution under the global null hypothesis.




Writing $\hat \beta(\lambda)$ for the lasso solution for a fixed value of $\lambda$, we need
the following facts summarized in \cite{significance:lasso} (ref Tibs2012?).

\begin{itemize}

  \item The vector valued function $\hat \beta(\lambda)$ is a
    continuous function of $\lambda$. For the lasso path, the
    coordinates of $\hat \beta(\lambda)$ are piecewise linear with
    changes in slope occurring at a finite number of $\lambda$ values
    referred to as \emph{knots}. The knots depend on the data and are
    usually written in order $\lambda_1 \geq \lambda_2 \geq \cdots
    \geq \lambda_r \geq 0$. We follow this convention. 

  \item The \emph{active set} $A_k$ is a set of indices of variables
    for which the corresponding coordinates of $\hat \beta(\lambda_k)$
    are potentially nonzero. Any variable with index not in $A_k$ has
    a zero coefficient in $\hat \beta(\lambda_k)$, but the converse is
    not true. \todo{is this equicorrelation set?}
  
  \item Path algorithms for computing lasso solutions proceed by
    fitting models at a grid of $\lambda$ values. The active set
    changes whenever $\lambda$ crosses a knot, and predictor variables
    can both enter and leave the active set. However, at the first two
    knots $\lambda_1$ and $\lambda_2$ no variable can leave the active
    set. So the first two knots correspond to the first two variables
    entering the model. 

\end{itemize}

\cite{significance:lasso} prove that under the null hypothesis that $A_k$
contains all the strong predictor variables, the distribution of a
test statistic $T_k \propto \lambda_k(\lambda_k - \lambda_{k+1})$ is
asymptotically Exp(1). In the lasso case we know a lot about the knots
and active set, but the group lasso picture is slightly more
complicated. For the group lasso, $\hat \beta(\lambda)$ does not have
piecewise linear components. To overcome this difficulty we will
restrict our attention to the first group of variables to enter the
active set since the analysis then follows almost exactly as for the
lasso.


The \emph{group lasso estimator} is a solution to the following convex
problem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
\label{eq:gsoln}
\displaystyle \hat \beta_\lambda = \argmin_{\beta \in \real^p} \frac{1}{2} \| y - X \beta \|_2^2 +
   \lambda \sum_{g=1}^G w_g \| \beta_g \|_2
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The parameter $\lambda \geq 0$ enforces sparsity in groups: for large
$\lambda$ most of the $\beta_g$ will be zero vectors. The weights
$w_g$ are usually taken to be $\sqrt {p_g}$ to normalize the penalty
across groups.  Note that this includes the usual lasso estimator as a
special case when all of the groups are of size 1, since then the
penalty term is the $L^1$-norm of $\beta$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% Related works %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo Fix references below

The group lasso estimator is discussed in (ref that guy's thesis) and
(ref Yuan and Lin).  An important extension is the sparse group lasso
(ref ???) which enforces sparsity in groups as well as sparsity of the
coefficients within the groups.  For a survey on group lasso and
related factor models see (ref???). \todo review some more literature
and add a few more references here if they seem worthwhile. 


Before considering the group lasso, we review some ingredients of the
proof for the lasso. Let $J = \{ 1, 2, \ldots, p \}$ index variables
and consider a stochastic process $f_{j,s} = sX_j^Ty$ defined on $T =
J \times [ -1, 1 ]$. This stochastic process is simply a collection of
linear combinations of $y$, hence it is Gaussian under the assumption
of Gaussian errors. The \emph{Karush-Kuhn-Tuckher (KKT) conditions}
(ref???) imply that $\lambda_1 = \max_j |X_j^Ty|$. By introducing the
sign variable $s$, we can remove the absolute value and write
$\lambda_1$ as the maximum of our Gaussian process 

\begin{equation}
\lambda_1 = \max_{(j,s)} f_{j,s}
\end{equation}

We have exhibited the first knot as the maximum of a Gaussian
process. We can do this for the second knot by introducing a new
process. Let $(j_1, s_1)$ be the maximizer so that $\lambda_1 =
s_1X_{j_1}^Ty$, and define 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
  f^{(j_1,s_1)}_{(j,s)} & = \frac{ sX_j^T y - s X_j^T X_{j_1} X_{j_1}^Ty  } { 1 -  ss_1 X_j^TX_{j_1}} \\
  & = \frac{ sX_j^T(I-P_{j_1}) y }{ 1 -  s_1 X_{j_1}^T X_js}
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $P_j$ is the projection onto the subspace spanned by $X_j$. We
can think of this as a ``residual process'' after regressing out the
maximum. Write $M = \max_{j \neq j_1, s} f^{(j_1,s_1)}_{(j,s)}$, the
maximum of this residual process. It can be shown from the KKT
conditions that $M  = \lambda_2$. To summarize, we have represented
the first two knots of the lasso solution path as the maxima of some
natural Gaussian processes. Distributional facts about Gaussian
processes now allow us to make conclusions about the distribution of
functions of the knots. 

\subsection{Group lasso}
To extend this argument to the group lasso we need to define Gaussian
processes that characterize the knots of the group lasso solution
path.  

\todo Either use the simplified argument in the case of equal weights
(equal group sizes), or finish adjusting the proof of $M = \lambda_2$
to include the weights and use that version. The proof can go in an
appendix. 

\todo Modify write-up to match notation with \cite{tests:adaptive} and
then paste it in here

\subsection{Relation to covariance test}

Consider the simple case for the first covariate to enter the model.
In this case the covariance test statistic is $T = \lambda_1(\lambda_1 - M)$.
\todo Does $M = \lambda_2$? Check proof, now that statistic has changed

This is Lemma 5 from \cite{significance:lasso}.

\begin{itemize}

  \item As in LTTT, $T = \lambda_1 (\lambda_1 - M)$ and $M = \lambda_2$
  \item Convergence to the limiting Exp(1) distribution is too slow
    \[
      \frac{P(\chi_k / w_g \geq m + t/m)}{P(\chi_k / w_g \geq m)} \to e^{-t} \text{ as } m \to \infty
    \]
    (when the group achieving $\lambda_1$ is group $g$ and has rank $k$)
  \item The limiting distribution only depends on $T$, but we also observe $M$
  \item Let's just try the ratio (conditional $\chi_k$ tail probability) evaluated at $T$ and $M$ (it works better)
  \item Going one step further, instead of using the approximation (see LTTT Proof of Lemma 5)
    \[
      \frac{M + \sqrt{M^2+4t}}{2} \approx M + \frac{t}{M}
    \]
    we can just use the left hand side
  \item For $T = \lambda_1(\lambda_1 - M)$ the left hand side simplifies to $\lambda_1$
  \item Now our p-value is
    \[
      \frac{P(\chi_k / w_g \geq \lambda_1)}{P(\chi_k / w_g \geq \lambda_2)}
    \]   
\end{itemize}


\subsection{Exact p-value calculation}

We now describe how to calculate our p-value, following the discussion of
group lasso in \cite{tests:adaptive}. For the rest of this section let
$g = \gstar$ be the index of
the group attaining the maximum on line 3 of Algorithm~\ref{algo:fs}.
%\lambda = \norm{X_g^Ty}_2/w_g$ be the value of the maximum, and
%$r = \text{rank}(X_g)$ be the rank of the maximizing group.
For a vector $u \in \real^p$, let $u_h$ denote the coordinates
of $u$ corresponding to the columns of group $h$ in the design matrix $X$.
We can rearrange the columns of $X$ to group these adjacently, so that
\[
u^TX^T = \begin{pmatrix} u_1^T X_1^T & u_2^T X_2^T & \cdots & u_G^TX_G^T \end{pmatrix}
\]
One step of the calculation will be to find an orthonormal basis for the
linear space $\vecsp_g = \{ u \in \real^p : u_g^T X_g^T y = 0, u_h = 0
\text{ for all } h \neq g \}$ so that we can project orthogonally to
this space. If $X_g$ is a single column and $X_g^Ty \neq 0$ (which
should be the case since $g$ maximizes the absolute value of this
quantity), then the
space is trivial and the desired orthogonal projection is the identity.
Otherwise, if $X_g$ has $p_g > 1$ columns, the space $\vecsp_g$ will
generally
have dimension $r-1$ for $r = \text{rank}(X_g)$. This holds except in
degenerate cases such as $X_g = 0$ or $r \ll p_g$, but these cases are not
likely to occur since $\norm{X_g^T y}_2/w_g$ is supposed to be maximized.
We proceed by forming the $n \times p_g$ matrix
$X_g - X_g X_g^T y y^T X_g/\norm{X_g^Ty}_2^2$ and performing Gram-Schmidt
on the first $r-1$ columns. For precise theorems showing that this will
generally work, see \cite{outerp:indep}. Next we take the basis
computed by Gram-Schmidt and form an $p \times r-1$ matrix, which we
denote $V_g$, by appending 0's in the coordinates corresponding to all
groups $h \neq g$ and to $p_g - r + 1$ columns in group $g$. With this
we are ready to define the projection

\begin{equation}
  \begin{aligned}
    P_g &= \Sigma XV_g (V_g^T X^T \Sigma X V_g)^\dagger V_g^TX^T \\
  \end{aligned}
\end{equation}


Define $U_g = X_g^Ty/(w_g\norm{X_g^Ty}_2)$.
\begin{algorithm}
  \caption{Computing p-value}
  \label{algo:pval}
  \begin{algorithmic}[1]
    \REQUIRE Response $y$, design matrix $X$, index $g$ of next group to enter
    \ENSURE P-value for testing if $g$ and all other remaining groups are noise
    \STATE $\lambda \gets \norm{X_g^Ty}_2/w_g$, $r \gets \text{rank}(X_g)$
    \IF{$X_g$ is a single column}
    \STATE $P \gets 0$ is a $p \times p$ matrix
    \STATE $\sigma^2_g \gets X_g^T \Sigma X_g$
    \ELSE
    \STATE Do that
    \ENDIF
    \RETURN $A$
  \end{algorithmic}
\end{algorithm}




\section{Simulations}
\label{sec:simulations}

\todo Clean up and reorganize \\
\todo Write a little more exposition \\
\todo Compare to AIC and other classical stopping rules? \\


\begin{figure}[h]
\begin{center}
\subfigure[Gaussian design]{
\label{fig:smallgauss}
\includegraphics[width=0.5\textwidth]{../figs/gaussian_size1_n100_p200_g200_k10_lower1p8_upper1p9.pdf}}
\hspace{-15pt}
\subfigure[Categorical design]{
\label{fig:smallcat}
\includegraphics[width=0.5\textwidth]{../figs/gaussian_size1_n100_p200_g200_k10_lower1p8_upper1p9_corr0p1.pdf}}
\caption{\small \it The left panel shows results of a simulation with
  an independent Gaussian design matrix and a signal vector having
  three groups with non-zero coefficients of sizes one, two, and
  three, and seven groups of noise variables with sizes varying from
  one to ten, for a total of 30 variables. For the right panel we
  repeated this setup, but group sizes corresponded to levels of
  categorical variables. The groups of size one were increased to two,
so there are 10 categorical variables with a total of 32 levels.}
\end{center}
\end{figure}


\input{../small_sim_selection.tex}

\input{../small_sim_estimation.tex}

\begin{figure}[h]
\begin{center}
\subfigure[Large Gaussian lasso]{
\label{fig:largelasso}
\includegraphics[width=0.5\textwidth]{../figs/gaussian_size5-10_n100_p200_g35_k10_lower1p8_upper1p9.pdf}}
\hspace{-15pt}
\subfigure[Large Gaussian group lasso]{
\label{fig:largegrplasso}
\includegraphics[width=0.5\textwidth]{../figs/gaussian_size5-10_n100_p200_g35_k10_lower1p8_upper1p9_corr0p1.pdf}}
\caption{\small \it The left panel shows results of a simulation with
  an independent Gaussian design matrix and a signal vector having
  three groups with non-zero coefficients of sizes one, two, and
  three, and seven groups of noise variables with sizes varying from
  one to ten, for a total of 30 variables. For the right panel we
  repeated this setup, but group sizes corresponded to levels of
  categorical variables. The groups of size one were increased to two,
so there are 10 categorical variables with a total of 32 levels.}
\end{center}
\end{figure}

\input{../large_sim_selection.tex}

The large simulation has 50 groups, 25 of size 1, 10 of size 5, 10 of
size 10, and 5 of size 15. Signal vectors were supported on random
choices of 10 of these groups (changing in each realization), with
non-zero signal magnitude around $\sqrt{2\log p}$ where $p = 250$
(roughly 3.3). The design matrix had 200 rows and the simulation
performed 100 realizations.



\section{Applications}
\label{sec:applications}
We now turn to applying forward stepwise and examining the behavior of
our test statistic in several unique settings, including a real data
example.


\subsection{Glinternet for hierarchical interactions}
\label{sec:glint}
In regression settings with many variables, considering models with pairwise interactions can drastically increase model complexity. \cite{glint} propose a method called \textsc{glinternet} to reduce the statistical complexity of this problem. The method imposes a strong hierarchical constraint on interactions (similar to that in \cite{bien:hierarchical}) where an interaction term can only be included if both its main effects are also included. They accomplish this by creating a design matrix with both main effects alone and also with groups including main effects with their first order interactions. Then they fit a group lasso model with the expanded design matrix. Because interaction terms only appear in groups along with their respective main effects, the hierarchy condition holds for the fitted model. We now consider a related procedure as an example problem, but first modify their method to simplify some parts. Let the expanded design matrix be given by
\begin{equation}
\label{eq:glintmat}
\tilde X = \begin{pmatrix} X_1 & \cdots & X_G & X_{1:2} & \cdots & X_{1:G} & X_{2:3} & \cdots & X_{(G-1):G}  \end{pmatrix}
\end{equation}
where $X_{g:h}$ is the submatrix including $X_g$, $X_h$, and all
$p_gp_h$  column multiples between columns in group $g$ and columns
in group $h$. For example, if
\[
X_g = \begin{pmatrix} X_{g1} & X_{g2} \end{pmatrix}, \quad
X_h = \begin{pmatrix} X_{h1} & X_{h2} \end{pmatrix}
\]
are two groups each containing two columns, then
\[
X_{g:h} = \begin{pmatrix} X_g & X_h & X_{g1} * X_{h1} & X_{g1} * X_{h2} & X_{g1} * X_{h1} & X_{g2} * X_{h2} \end{pmatrix}
\]
where * denotes the pointwise product (Hadamard product) of vectors
(the $i$th entry of $X_{g1} * X_{h1}$ is the $i$th entry of $X_{g1}$ times
the $i$th entry of $X_{h1}$). Note that this expanded matrix has
$\tilde G = G + \binom{G}{2} = O(G^2)$ groups. We refer to the first
$G$ of these as main effects or main effect groups, and the remaining
as interaction groups. Finally, instead of fitting a
model by group lasso, we use forward stepwise on the expanded design
matrix. The overlapping groups still guarantee that our fitted model will
satisfy the strong hierarchy condition. 

%However, we must first modify our implementation of forward stepwise to prevent the possibility of including main effect groups after the main effects are already included in the model as part of an interaction group. To do this we compute the residual using the whole model in each step, rather than by subtracting the effect of the current group being included. For a set of group indices $A \subseteq \{ 1, \ldots G \}$ we write $X_A$ for the submatrix of the design matrix containing all the groups in $A$.
% \begin{algorithm}
% \DontPrintSemicolon
% \caption{\small \it Modification to compute residual from full model}
% \BlankLine
% \CommentSty{\# Everything the same as before, additionally:}\;
% %  $P_A \gets I - X_AX_A^\dagger$\;
% %  $r_s \gets P_A r_{s-1}$\;
% \If{$g^*$ is an interaction group}{
%   $g_1, g_2 \gets$ main effects groups of $g^*$\;
%   A^c \gets A^c \slash \{ g_1, g_2 \}\;
% }%\;
% %$r_s \gets y - \text{lsfit}(X_A, y)$\;
% \BlankLine
% \label{algo:fs:full}
% \end{algorithm}
% \begin{algorithm}
% \DontPrintSemicolon
% \KwData{An $n$ vector $y$ and $n \times p$ matrix $X$ of $G$ groups of variables}
% \KwResult{Active set $A$ of variable groups included in the model}
% \SetKwFunction{lsfitResidual}{lsfitResidual}
% \caption{\small \it Forward stepwise for glinternet}
% \BlankLine
% Compute design matrix~(\ref{eq:glintmat}) and expanded grouping $\tilde G$\;
% $A \gets \emptyset$\;
% $A^c \gets \{ 1, \ldots, G \}$\;
% $r_0 \gets y$\;
% \For{$s \gets 1$ \KwTo $steps$}{
%   $g^* \gets \argmax_{g \in A^c} \{ \norm{X_g^T r_{s-1}}_2 / w_g \}$\;
%   $P \gets I - X_{g^*}X_{g^*}^\dagger$\;
%   $A \gets A \cup \{ g^* \}$\;
%   $A^c \gets A^c \backslash \{ g^* \}$\;
%   \For{$g \in A^c$}{
%     $X_g \gets P X_g$\;
%   }
%   $r_s \gets P r_{s-1}$\;
% }
% \KwRet{$A$}
% \label{algo:fsgl}
% \end{algorithm}

To demonstrate this method by simulation, we constructed signals which have
the first $k/3$ main effects nonzero but with no interactions, and the
remaining $2k/3$ nonzero main effects are matched to each other to form
interactions.
This way each nonzero main effect with any nonzero interactions has
exactly one nonzero interaction. Furthermore, each nonzero interaction
coefficient has been inflated to be larger than the corresponding main
effect coefficients. This special case is favorable for our algorithm,
but our purpose here is merely to demonstrate the flexibility of the
hypothesis test and not to propose a general procedure for models with
interactions.

Results are shown in Figure~\ref{fig:glint}. The left
panel shows average power of forward stepwise. Power is calculated
using the group structure we impose, and not in terms of the original
main effects and interactions. However, the dotted line shows a more
forgiving definition of power where we are rewarded for discovering part
of a nonzero group, i.e. for discovering only one main effect from a
true interaction group. In the right panel we see that the global null
result still holds (red vertical lines), and that with a signal the
p-value is small until the signal has been recovered (blue). The
interaction power is the proportion of nonzero interaction groups
that were discovered and is labeled ``Int. Power.''

\begin{figure}
\begin{center}
\subfigure[Forward stepwise on glinternet]{
\label{fig:glint:fwd}
\includegraphics[width=0.5\textwidth]{../figs/fwdstep/gauss_glint_p20_n200_lower2p9_upper3p6.pdf}}
\hspace{-15pt}
\subfigure[P-value with glinternet]{
\label{fig:glint:pval}
\includegraphics[width=0.5\textwidth]{../figs/gaussian_size1_n200_p40_g40_k6_lower2p5_upper3p9_glint.pdf}}
\caption{\small \it The left panel shows the power of forward stepwise
for various sparsity levels. The right panel shows our p-value marginally
over each step, with red indicating the global null case and blue
indicating a signal with $k=6$ nonzero groups.}
\label{fig:glint}
\end{center}
\end{figure}


\subsection{Generalized additive model selection}
\todo Finish code \\
\todo Write section \\

\subsection{Real data example}
\todo Code simulation with HIVdb data \\
\todo Write section \\



\section{Discussion}
\label{sec:discuss}

\todo Brief summary \\
\todo After finising everything else, move some points of discussion
here \\
\todo Mention ongoing work, like tracking all the constraints to
get an exact p-value at every step. \\

\newpage

\bibliographystyle{ims}
%\bibliographystyle{acmtrans-ims}
\bibliography{paper}

\newpage

\appendix
\section{Derivation of closed forms of statistic}

\todo Clean this section \\
\todo Make $M=\lambda_2$ a special case corollary \\

For the sake of completeness this appendix provides full derivations
of closed forms for the quantities required to compute our
statistic. We require two facts from \cite{tests:adaptive}

A point $\eta \in \K$ maximizes $f_\eta$ over a convex
set $\K$ if and only if the following conditions hold: 
\begin{equation}
\label{eq:maxcond}
\grad f_{|T_{\eta}\K} = 0, \quad
\tf^{\eta}_{\eta} \geq \V^+_{\eta}, \quad
\tf^{\eta}_{\eta} \leq \V^-_{\eta}, \quad \text{and} \quad
\V^0_{\eta} \leq 0.
\end{equation}
The same equivalence holds true even when $\K$ is only locally
convex. 

Write $M^{\pm}_{\eta, h}$ and $M^0_{\eta, h}$ as the corresponding suprema and infima over the restricted parameter set $S_h$. Note that the characterization of the global maximizer (Lemma 1, general paper) implies $M^0_{\eta, h} \leq 0$ and $M^+_{\eta, h} \leq M^-_{\eta, h}$ for all $h \neq g$. This will be used below to eliminate some degenerate cases of the optimization sub-problem on each $S_h$.

Now fix $h$, let $P_g = X_gX_g^T$ and define
\begin{align*}
a_h &= w_h^{-1} X_h^T (I-P_g) y \\
b_h &= w_g w_h^{-1} X_h^T P_g y / \norm{X_g^T y} \\
c_h &= a_h^T b_h / (\norm{a_h} \norm{b_h}) \\
K_h   &= \{ x : \norm{x} = 1, b_h^Tx > w \}
\end{align*}
Note that $K_h = \emptyset$ if and only if $\norm{b_h} < w$. We will consider two cases. First, suppose $\norm{b_h} > w$. In this case we must rule out one sub-case, when $a_h$ is not in the polar cone of $K_h$. If this occurs, then on at least one side of the boundary of $K_h$ we have $a_h^Tx > 0$ there, so that the infimum inside $K$ is $-\infty$ but the supremum outside $K$ is $+\infty$. This violates the characterization of the global maximizer of the original process as described above, so this case cannot occur.

Now, if $a_h$ is in the polar cone of $K_h$ (and ruling out the probability zero case where $a_h^Tx = 0$ on the boundary of $K_h$), then the numerator $a_h^Tx < 0$ on $K_h$ so there is a finite positive infimum in the interior $K_h$. Similarly there is a finite positive supremum on the interior of $K_h^c$ (the numerator is negative near the boundary of $K_h$ by continuity).

To attain the infimum on $K_h$ requires making $x$ close to $b_h$ and simultaneously making the numerator closer to zero, so $x$ should be on the side of $b_h$ that is closer to $a_h$. To attain the supremum on $K_h^c$ requires making $x$ simultaneously close to $a_h$ (so that $a_h^Tx > 0$) and $b_h$ (to make the denominator small). In summary, both the infimum and supremum are attained with $x$ between $a_h$ and $b_h$, when the angle $\theta$ between $a_h$ and $b_h$ is larger than both the angle $\psi$ between $a_h$ and $x$ and the angle $\phi$ between $x$ and $b_h$. This allows the simplification $\phi = \theta - \psi$. This is easier to verify for the case when $\norm{b_h} < w$.

We have verified that solving both cases requires finding the maxima of the trigonometric form $\norm{a_h} \cos (\psi) / (w - \norm{b_h} \cos(\theta - \psi))$ of the linear fraction on the interiors of $K_h$ (if it is nonempty) and $K_h^c$. The derivative is proportional to
\[
\frac{\norm{b_h} \sin(\theta) - w \sin(\psi)}{w - \norm{b_h} \cos(\theta - \psi)}
\]
Since we have ruled out the boundary of $K_h$, we can ignore the denominator and focus on critical points corresponding to the angles $\psi^{\pm}$ (symmetric about $\pi/2$) where $\sin(\psi^{\pm}) = (\norm{b_h} / w) \sin (\theta)$. That is,
\[
\psi^{\pm} \in \arcsin \frac{\norm{b_h}}{w} \sqrt{1-c_h^2}
\]
Note that these critical points only exist if $\sin(\theta) < w/\norm{b_h}$, and since we have argued that the infimum and supremum are attained it follows that this condition is met in our case. Let $\psi^+$ denote the smaller of the two angles. If $K_h$ is nonempty, then $\psi^-$ gives the infimum in $K_h$ and $\psi^+$ the supremum on $K_h^c$. This is necessary since if $x$ is in $K_h$ then its angle from $a_h$ is larger than $\pi/2$ because $a_h$ is in the polar cone of $K_h$. If $K_h$ is empty, then the supremum is still attained at $\psi^+$ since the numerator is negative at $\psi^- > \pi/2$ and the denominator is always positive.

Finally we calculate the values of the linear fraction at $\psi^{\pm}$. Let us first record some facts:
\begin{align*}
0 < \psi^+ < \pi/2, \quad \psi^- = \pi - \psi^+ \\
\cos(\theta) = c_h, \quad \sin(\theta) = \sqrt{1-c_h^2} \\
\sin(\psi^+) = \sin(\psi^-) = \frac{\norm{b_h}}{w} \sin(\theta) \\
\cos(\psi^+) = -\cos(\psi^-) = \sqrt{1 - (\norm{b_h}/w)^2(1-c_h^2)}
\end{align*}
Using the angle difference formula,
\begin{align*}
\cos(\theta - \psi^+) & = \frac{\norm{b_h}}{w} (1-c_h^2) +  c_h \cos(\psi^+) \\
& = \frac{\norm{b_h}}{w} (1-c_h^2) +  c_h \cos(\psi^+)
\end{align*}
Hence
\begin{align*}
M^+_{\eta, h} & = \frac{ \norm{a_h} c_h }{ w - (\norm{b_h}^2/w) (1-c_h^2) - \norm{b_h} c_h \cos(\psi^+) } \\
% & = 
% & = \frac{ w \norm{a} a_h^Tb_h / \norm{b_h} }{ w^2 - \norm{a}\norm{b}^2 + (a^Tb)/\norm{a} - a^Tb \cos \psi^+ }
\end{align*}
And $M^-_{\eta, h}$ is $\infty$ when $\norm{b_h} < w$ and otherwise given by
\begin{align*}
M^+_{\eta, h} & = \frac{ \norm{a_h} c_h }{ w - (\norm{b_h}^2/w) (1-c_h^2) + \norm{b_h} c_h \cos(\psi^+) } \\
\end{align*}

I haven't been able to algebraically simplify these yet to a form that allows my $\lambda_2$ proof below to go through. The version below just assumes $\norm{b_h} < w = 1$.

\hrule
\hrule
We can rewrite this by rationalizing the denominator:
\[
M =
\frac
{ a_h^T b_h +  \sqrt{ a_h^T a_h (1-b_h^Tb_h) + (a_h^T b_h)^2 }  }
{ 1-b_h^Tb_h }
\]
Leaving $M$ in this form we now consider $\lambda_2$. The KKT conditions for the group lasso problem give (directly from Yuan and Lin)
\begin{align*}
\norm{ X_g^T(y - X \beta) } & \leq \lambda w_g \quad \forall \beta_g \neq 0 \\
\beta_g & = \left( 1 - \frac{ \lambda w_g }{ \norm{S_g} } \right)_+ S_g
\end{align*}
where $S_g = X_g^T(y - X \beta_{-g})$. These expressions simplify for $\lambda_2 \leq \lambda < \lambda_1$. Letting $g$ be the index of the first group to enter we now have $S_g = X_g^Ty$ 
\begin{align*}
\beta_g & = \left( 1 - \frac{ \lambda w_g }{ \norm{X_g^Ty} } \right) X_g^Ty \\
\lambda w_h & \geq \norm{ X_h^Ty - X_h^T P_g y (1-\lambda w_g / \norm{X_g^Ty})  } \quad \forall h \neq g
\end{align*}
Now let $\lambda = \lambda_2$, so the inequality above is strict for all other groups and equality is attained for the second group to enter. Hence $\lambda_2$ satisfies
\begin{align*}
\lambda_2 & = \max_{h \neq g} 
\norm{ X_h^T(I - P_g) y - \lambda_2 ( w_g X_h^T P_g y / \norm{X_g^Ty} )  } / w_h\\
& = \max_{h \neq g} \norm{ a_h - \lambda_2 b_h } 
\end{align*}

Write $\lambda_{2,h}$ as the positive root of the quadratic equation obtained by squaring the norm, so $\lambda_{2,h}^2 = \norm{ a_h - \lambda_2 b_h }^2$. Then $\lambda_2 = \max_{h \neq g} \lambda_{2,h}$. Solving this (note that $b_h^Tb_h < 1$), we find
\[
\lambda_{2,h} = \frac
{ a_h^T b_h +  \sqrt{ a_h^T a_h (1-b_h^Tb_h) + (a_h^T b_h)^2 }  }
{ 1-b_h^Tb_h }
\]
Hence when $h$ is the index of the second group to enter, we have $\lambda_2 = \lambda_{2,h} = M$.

\end{document}

